{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzwxxQPJxmIJ",
        "outputId": "dbb21be5-99f0-4d54-acd8-83605c28beb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import  word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import requests\n",
        "import io\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "Dl55Qljaxoul"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e10rjaFixrdR",
        "outputId": "73578ed7-036f-4259-cec2-f80e94d493e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "zKSQ1t0IEG6_",
        "outputId": "e6e76e00-f79d-477a-b65d-8562351e71d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d497cf56-99ed-46d5-bfd1-a1320f8ba651\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d497cf56-99ed-46d5-bfd1-a1320f8ba651\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving disaster-tweets.csv to disaster-tweets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "YqvbMNa4xtBV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_data(path):\n",
        "    data = []\n",
        "\n",
        "    stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
        "    custom_stopwords = set(stopwords_list.decode().splitlines())\n",
        "    custom_stopwords.add(\"amp\")\n",
        "\n",
        "    nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "    all_stopwords = nltk_stopwords.union(custom_stopwords)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            text = row[\"text\"].lower()\n",
        "            text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "            text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "            tokens = word_tokenize(text)\n",
        "            tagged = pos_tag(tokens)\n",
        "\n",
        "            words = [\n",
        "                lemmatizer.lemmatize(w, get_wordnet_pos(pos))\n",
        "                for w, pos in tagged\n",
        "                if w not in all_stopwords and len(w) > 2\n",
        "            ]\n",
        "            data.append((words, int(row[\"target\"])))\n",
        "    return data"
      ],
      "metadata": {
        "id": "Yaz_rZCcxunJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data, max_words=10000):\n",
        "    freq = Counter()\n",
        "    for words, _ in data:\n",
        "        freq.update(words)\n",
        "    most_common = freq.most_common(max_words)\n",
        "    vocab = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
        "    return vocab\n",
        "\n",
        "def vectorize(data, vocab):\n",
        "    vectors = []\n",
        "    labels = []\n",
        "    for words, label in data:\n",
        "        vec = [0] * len(vocab)\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                vec[vocab[word]] += 1\n",
        "        vectors.append(vec)\n",
        "        labels.append(label)\n",
        "    return vectors, labels"
      ],
      "metadata": {
        "id": "Lq2hS_9Ryprr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self, num_classes, num_words, pseudocount=1):\n",
        "        self.num_classes = num_classes\n",
        "        self.num_words = num_words\n",
        "        self.pseudocount = pseudocount\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        self.priors = [0] * self.num_classes\n",
        "        self.likelihoods = [[0]*self.num_words for _ in range(self.num_classes)]\n",
        "        class_counts = [0] * self.num_classes\n",
        "        word_counts = [[0]*self.num_words for _ in range(self.num_classes)]\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            self.priors[y] += 1\n",
        "            for i, count in enumerate(x):\n",
        "                word_counts[y][i] += count\n",
        "            class_counts[y] += sum(x)\n",
        "\n",
        "        total = len(Y)\n",
        "        self.priors = [math.log(p / total) for p in self.priors]\n",
        "\n",
        "        for c in range(self.num_classes):\n",
        "            denom = class_counts[c] + self.num_words * self.pseudocount\n",
        "            for i in range(self.num_words):\n",
        "                num = word_counts[c][i] + self.pseudocount\n",
        "                self.likelihoods[c][i] = math.log(num / denom)\n",
        "\n",
        "    def predict(self, x):\n",
        "        scores = []\n",
        "        for c in range(self.num_classes):\n",
        "            score = self.priors[c]\n",
        "            for i in range(self.num_words):\n",
        "                score += x[i] * self.likelihoods[c][i]\n",
        "            scores.append(score)\n",
        "        return scores.index(max(scores))"
      ],
      "metadata": {
        "id": "2Wng0m2oyrr7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_part_a(path, runs=3):\n",
        "    data = load_and_clean_data(path)\n",
        "    accuracies = []\n",
        "    for _ in range(runs):\n",
        "        random.shuffle(data)\n",
        "        split = int(0.8 * len(data))\n",
        "        train_data = data[:split]\n",
        "        test_data = data[split:]\n",
        "\n",
        "        vocab = build_vocab(train_data)\n",
        "        X_train, Y_train = vectorize(train_data, vocab)\n",
        "        X_test, Y_test = vectorize(test_data, vocab)\n",
        "\n",
        "        model = MultinomialNaiveBayes(2, len(vocab))\n",
        "        model.fit(X_train, Y_train)\n",
        "\n",
        "        correct = 0\n",
        "        for x, y in zip(X_test, Y_test):\n",
        "            pred = model.predict(x)\n",
        "            if pred == y:\n",
        "                correct += 1\n",
        "        acc = correct / len(Y_test)\n",
        "        print(f\"Accuracy: {acc*100:.2f}%\")\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    avg = sum(accuracies) / len(accuracies)\n",
        "    print(f\"Prosečna tačnost u {runs} pokretanja: {avg*100:.2f}%\")"
      ],
      "metadata": {
        "id": "0BRpu2YVytQ_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_words(path):\n",
        "    data = load_and_clean_data(path)\n",
        "    pos_counter = Counter()\n",
        "    neg_counter = Counter()\n",
        "\n",
        "    for words, label in data:\n",
        "        word_freq = Counter(words)\n",
        "        if label == 1:\n",
        "            pos_counter.update(word_freq)\n",
        "        else:\n",
        "            neg_counter.update(word_freq)\n",
        "\n",
        "    print(\"\\nTop 5 reči u pozitivnim tvitovima:\")\n",
        "    for word, count in pos_counter.most_common(5):\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    print(\"\\nTop 5 reči u negativnim tvitovima:\")\n",
        "    for word, count in neg_counter.most_common(5):\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    #LR metrika\n",
        "    lr_scores = {}\n",
        "    for word in pos_counter:\n",
        "        if pos_counter[word] >= 10 and neg_counter[word] >= 10:\n",
        "            lr_scores[word] = pos_counter[word] / neg_counter[word]\n",
        "\n",
        "    top5_high = sorted(lr_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    top5_low = sorted(lr_scores.items(), key=lambda x: x[1])[:5]\n",
        "\n",
        "    print(\"\\nTop 5 reči sa NAJVEĆOM LR metrikom:\")\n",
        "    for word, score in top5_high:\n",
        "        print(f\"{word}: {score:.2f}\")\n",
        "\n",
        "    print(\"\\nTop 5 reči sa NAJMANJOM LR metrikom:\")\n",
        "    for word, score in top5_low:\n",
        "        print(f\"{word}: {score:.2f}\")\n"
      ],
      "metadata": {
        "id": "8mqHLTWEyu3C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_part_a(\"disaster-tweets.csv\")\n",
        "    analyze_words(\"disaster-tweets.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnV3YA0nywgP",
        "outputId": "7e36b56e-2be0-440a-aa4b-582bc770d021"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.19%\n",
            "Accuracy: 78.66%\n",
            "Accuracy: 79.51%\n",
            "Prosečna tačnost u 3 pokretanja: 79.12%\n",
            "\n",
            "Top 5 reči u pozitivnim tvitovima:\n",
            "kill: 159\n",
            "news: 151\n",
            "bomb: 132\n",
            "disaster: 122\n",
            "california: 115\n",
            "\n",
            "Top 5 reči u negativnim tvitovima:\n",
            "body: 119\n",
            "love: 117\n",
            "time: 109\n",
            "bag: 109\n",
            "day: 104\n",
            "\n",
            "Top 5 reči sa NAJVEĆOM LR metrikom:\n",
            "kill: 8.37\n",
            "train: 5.61\n",
            "report: 5.50\n",
            "fire: 5.06\n",
            "fatal: 4.58\n",
            "\n",
            "Top 5 reči sa NAJMANJOM LR metrikom:\n",
            "love: 0.13\n",
            "scream: 0.17\n",
            "feel: 0.19\n",
            "play: 0.20\n",
            "wreck: 0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Najčešće korišćene reči u pozitivnim i negativnim tvitovima pokazuju jasnu razliku u temama koje se obrađuju.\n",
        "U pozitivnim tvitovima (relevantnim za katastrofe) preovlađuju reči poput \"fire\", \"kill\", \"news\", \"bomb\", \"disaster\", koje jasno asociraju na hitne slučajeve, nasilje i vanredne događaje.\n",
        "Nasuprot tome, u negativnim (nerelevantnim) tvitovima najzastupljenije su neutralne reči, često prisutne u svakodnevnoj komunikaciji, kao što su \"get\", \"like\", \"new\", \"one\", \"make\", što sugeriše da ti tvitovi ne sadrže elemente povezane sa katastorfama.\n",
        "\n",
        "LR metrika se koristi kao mera koja pokazuje koliko je neka reč indikativna za pozitivne ili negativne tvitove.\n",
        "Računa se po formuli: LR(reč) = broj pojavljivanja reči u pozitivnim tvitovima / broj pojavljivanja reči u negativnim tvitovima.\n",
        "U obzir se uzimaju samo reči koje se pojavljuju najmanje deset puta u oba korpusa, kako bi se izbegla pristrasnost izazvana retkim rečima.\n",
        "\n",
        "Reči sa NAJVEĆOM LR metrikom:'kill', 'train', 'report', 'fatal', 'evacuation', su reči jake indikacije za katastrofe, nesreće i vanredne situacije. Značajno se češće pojavljuju u pozitivnim tvitovima (relevantnim za katastrofe) nego u negativnim, pa imaju visoke LR vrednosti.\n",
        "Reči sa NAJMANJOM LR metrikom:'feel', 'love', 'scream', 'play', 'wreck', predstavljaju svakodnevne izraze ili izraze emocija koji se češće koriste u opštim, nerelevantnim tvitovima. Njihovo prisustvo sugeriše da tvit verovatno nije vezan za katastrofu, pa su korisni negativni indikatori.\n",
        "LR metrika je korisna jer ne meri samo učestalost reči, već i koliko doprinosi razdvajanju klasa. Reči sa visokom LR vrednošću pomažu modelu da tačnije klasifikuje tvit kao pozitivan, dok one sa niskom vrednošću mogu doprineti negativnoj klasifikaciji. U poređenju sa prostom učestalošću, LR pruža bolji uvid u informativnost reči za klasifikaciju.\n"
      ],
      "metadata": {
        "id": "v-DEvMo0jlkj"
      }
    }
  ]
}